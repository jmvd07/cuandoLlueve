{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción al aprendizaje supervisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer, StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actividades:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset rain_teodelina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) agregar features que aporten valor al dataset  \n",
    "\n",
    "2) Analizar features data / target \n",
    "\n",
    "3) dividir dataset (training, validation, test)    \n",
    "\n",
    "4) analizar y elegir el modelo mas apropiado, entrenarlo y analizar resultados  \n",
    "\n",
    "5) combinar clasificadores y analizar resultados  \n",
    "\n",
    "6) evaluar predicciones de los diferentes modelos  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rain</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1978-01-01</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978-01-02</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978-01-03</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978-01-04</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978-01-05</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rain\n",
       "date            \n",
       "1978-01-01     0\n",
       "1978-01-02     0\n",
       "1978-01-03     0\n",
       "1978-01-04     0\n",
       "1978-01-05     0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rain_t = pd.read_csv(\"rain_teodelina.csv\", parse_dates = [\"date\"], index_col=[0])\n",
    "rain_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15034.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.331914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.451975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>220.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               rain\n",
       "count  15034.000000\n",
       "mean       3.331914\n",
       "std       11.451975\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        0.000000\n",
       "max      220.000000"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rain_t.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values_count_t = rain_t.isnull().sum()\n",
    "missing_values_count_t[missing_values_count_t > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_t['mes'] = rain_t.index.month\n",
    "rain_t['año'] = rain_t.index.year\n",
    "#lluvias['mesHid'] = (lluvias.index.month+5)%12+1\n",
    "rain_t['añoHid'] = (rain_t.index + dt.timedelta(days=181))\n",
    "rain_t['mesHid'] = rain_t.añoHid.dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rain</th>\n",
       "      <th>mes</th>\n",
       "      <th>año</th>\n",
       "      <th>añoHid</th>\n",
       "      <th>mesHid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15034.000000</td>\n",
       "      <td>15034.000000</td>\n",
       "      <td>15034.000000</td>\n",
       "      <td>15034</td>\n",
       "      <td>15034.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15034</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1989-12-27 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1978-07-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-08-28 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.331914</td>\n",
       "      <td>6.503193</td>\n",
       "      <td>1998.082413</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.526739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.451975</td>\n",
       "      <td>3.456608</td>\n",
       "      <td>11.881857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.442680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1978.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>1988.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1998.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>220.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2019.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                rain           mes           año               añoHid  \\\n",
       "count   15034.000000  15034.000000  15034.000000                15034   \n",
       "unique           NaN           NaN           NaN                15034   \n",
       "top              NaN           NaN           NaN  1989-12-27 00:00:00   \n",
       "freq             NaN           NaN           NaN                    1   \n",
       "first            NaN           NaN           NaN  1978-07-01 00:00:00   \n",
       "last             NaN           NaN           NaN  2019-08-28 00:00:00   \n",
       "mean        3.331914      6.503193   1998.082413                  NaN   \n",
       "std        11.451975      3.456608     11.881857                  NaN   \n",
       "min         0.000000      1.000000   1978.000000                  NaN   \n",
       "25%         0.000000      3.250000   1988.000000                  NaN   \n",
       "50%         0.000000      7.000000   1998.000000                  NaN   \n",
       "75%         0.000000     10.000000   2008.000000                  NaN   \n",
       "max       220.000000     12.000000   2019.000000                  NaN   \n",
       "\n",
       "              mesHid  \n",
       "count   15034.000000  \n",
       "unique           NaN  \n",
       "top              NaN  \n",
       "freq             NaN  \n",
       "first            NaN  \n",
       "last             NaN  \n",
       "mean        6.526739  \n",
       "std         3.442680  \n",
       "min         1.000000  \n",
       "25%         4.000000  \n",
       "50%         7.000000  \n",
       "75%        10.000000  \n",
       "max        12.000000  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rain_t.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "AcumMensual = pd.pivot_table(rain_t, values='rain', index=['año'],columns=['mes'], aggfunc=np.sum, margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>mes</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>año</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>217.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>110.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>21.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>297.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>228.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>113.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>203.0</td>\n",
       "      <td>365.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>83.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>331.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>96.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>134.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>108.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>367.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>191.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>172.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>141.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>130.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>86.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>219.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>211.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>62.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>93.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>121.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>142.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>210.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>234.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>168.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>116.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>140.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>167.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>93.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>328.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>364.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>41.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>229.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>209.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>158.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>92.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>147.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>123.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>69.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>359.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>209.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>160.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>115.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>33.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>293.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>153.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>235.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>173.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>368.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>346.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>314.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>153.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>66.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>182.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "mes      1      2      3      4      5      6      7      8      9      10  \\\n",
       "año                                                                          \n",
       "1978  217.0  140.0  204.0   77.0   29.0   16.0   68.0   22.0  303.0  132.0   \n",
       "1979  110.0  173.0  166.0   89.0   14.0   47.0   17.0    9.0   29.0   69.0   \n",
       "1980   21.0  113.0  214.0  174.0   30.0   54.0   11.0    5.0   17.0  173.0   \n",
       "1981  297.0  108.0  179.0   92.0   82.0   35.0   16.0    1.0   47.0  103.0   \n",
       "1982  228.0  199.0  103.0  111.0    3.0   26.0   13.0    0.0  107.0   31.0   \n",
       "1983  113.0  148.0   52.0   86.0   50.0    6.0    5.0   32.0   11.0  202.0   \n",
       "1984  203.0  365.0  113.0   39.0   30.0   12.0   12.0   24.0   74.0  160.0   \n",
       "1985   83.0  149.0   61.0   82.0   62.0    0.0   85.0   36.0   71.0  158.0   \n",
       "1986  331.0   50.0   37.0  139.0   22.0   40.0    6.0   31.0   79.0  150.0   \n",
       "1987   96.0  224.0  121.0   75.0   42.0    4.0   86.0   27.0   11.0  144.0   \n",
       "1988  108.0   69.0  367.0   41.0    0.0    3.0   35.0    0.0   60.0  105.0   \n",
       "1989  191.0  100.0  136.0  107.0   62.0   41.0   34.0   20.0   31.0   59.0   \n",
       "1990  141.0  155.0  261.0  213.0  105.0    0.0   48.0   30.0   77.0  141.0   \n",
       "1991  130.0  162.0  196.0  140.0   70.0  118.0   43.0   68.0   61.0   89.0   \n",
       "1992   86.0   38.0  154.0   81.0   22.0   72.0   18.0   87.0   66.0   60.0   \n",
       "1993  219.0   80.0   94.0  292.0  129.0   87.0    0.0   60.0  100.0  213.0   \n",
       "1994   62.0   87.0   19.0  126.0   57.0   38.0   45.0   41.0   26.0  101.0   \n",
       "1995   93.0   48.0  197.0  238.0   55.0   33.0    6.0    1.0   24.0  161.0   \n",
       "1996  121.0  152.0   61.0  166.0   33.0    0.0    7.0   15.0   36.0   97.0   \n",
       "1997  210.0   56.0   59.0   50.0   96.0   72.0   22.0    9.0   10.0  180.0   \n",
       "1998  168.0  131.0   91.0  174.0  108.0   10.0   56.0   15.0   22.0   84.0   \n",
       "1999  140.0  118.0  330.0  151.0   16.0   34.0   12.0   54.0   40.0  122.0   \n",
       "2000   93.0  177.0  109.0  239.0  298.0   23.0    4.0   52.0   61.0  212.0   \n",
       "2001  328.0   47.0  364.0  151.0   24.0   12.0    7.0  119.0  186.0  289.0   \n",
       "2002  188.0   46.0  208.0   85.0  113.0    6.0   29.0   91.0   17.0  252.0   \n",
       "2003   41.0  202.0  128.0  205.0   37.0   19.0   99.0   24.0   19.0   45.0   \n",
       "2004  229.0   25.0   74.0  142.0   95.0    5.0   71.0   43.0    0.0   89.0   \n",
       "2005  158.0   79.0  144.0   58.0    3.0   18.0   52.0   43.0   54.0   58.0   \n",
       "2006   92.0   82.0   87.0   87.0    9.0   24.0   27.0    0.0   22.0  198.0   \n",
       "2007  147.0  274.0  294.0   64.0   49.0   18.0    7.0    7.0   84.0   81.0   \n",
       "2008  123.0  178.0  122.0    4.0    2.0   22.0   47.0    0.0   56.0   98.0   \n",
       "2009   69.0  118.0   56.0   50.0   45.0    0.0   40.0    5.0  161.0   74.0   \n",
       "2010  209.0  138.0   89.0   45.0   88.0   13.0   10.0    7.0  120.0   89.0   \n",
       "2011  160.0  134.0   87.0  120.0   59.0   16.0    7.0    4.0   24.0  103.0   \n",
       "2012  115.0  288.0  176.0   21.0   80.0    0.0    0.0   83.0   79.0  377.0   \n",
       "2013   33.0   56.0  178.0  110.0  150.0   24.0    0.0    0.0   33.0   68.0   \n",
       "2014  293.0  329.0  154.0  266.0  145.0   64.0   16.0    0.0  142.0  174.0   \n",
       "2015  235.0  167.0  194.0  190.0  116.0   37.0  120.0  255.0   46.0  128.0   \n",
       "2016  173.0  285.0   50.0  368.0   15.0   73.0   23.0    6.0   83.0  178.0   \n",
       "2017  314.0  217.0  286.0  318.0  141.0   30.0   53.0  118.0  170.0  187.0   \n",
       "2018   66.0   60.0   28.0  232.0  167.0   35.0   47.0   74.0   60.0  151.0   \n",
       "\n",
       "mes      11     12  \n",
       "año                 \n",
       "1978  235.0   50.0  \n",
       "1979  190.0  120.0  \n",
       "1980  112.0  107.0  \n",
       "1981  147.0   81.0  \n",
       "1982  133.0   52.0  \n",
       "1983  127.0   40.0  \n",
       "1984  105.0   81.0  \n",
       "1985  147.0   75.0  \n",
       "1986  131.0   16.0  \n",
       "1987  159.0  134.0  \n",
       "1988   26.0   96.0  \n",
       "1989   98.0  172.0  \n",
       "1990  127.0  119.0  \n",
       "1991  111.0  360.0  \n",
       "1992  200.0  115.0  \n",
       "1993   78.0  211.0  \n",
       "1994   41.0  120.0  \n",
       "1995   87.0  182.0  \n",
       "1996  102.0  142.0  \n",
       "1997  115.0  234.0  \n",
       "1998  167.0  116.0  \n",
       "1999   60.0  167.0  \n",
       "2000  214.0   42.0  \n",
       "2001  103.0  154.0  \n",
       "2002  234.0  240.0  \n",
       "2003  101.0  129.0  \n",
       "2004  106.0  209.0  \n",
       "2005  163.0   19.0  \n",
       "2006   67.0  165.0  \n",
       "2007  112.0   89.0  \n",
       "2008  146.0   67.0  \n",
       "2009  191.0  359.0  \n",
       "2010   18.0   63.0  \n",
       "2011   69.0    3.0  \n",
       "2012  172.0  129.0  \n",
       "2013  266.0  131.0  \n",
       "2014  250.0  153.0  \n",
       "2015  202.0  149.0  \n",
       "2016  135.0  346.0  \n",
       "2017   56.0  153.0  \n",
       "2018  138.0  182.0  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumas_short = AcumMensual.drop(['All'], axis=1)\n",
    "sumas_short = sumas_short.drop([2019,'All'], axis=0)\n",
    "sumas_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitation = rain_t.rain\n",
    "\n",
    "consecutive_dry = [1 if data == 0 else 0 for data in precipitation]\n",
    "for i in range(1, len(consecutive_dry)):\n",
    "    if consecutive_dry[i] == 1:\n",
    "        consecutive_dry[i] += consecutive_dry[i - 1]\n",
    "        \n",
    "consecutive_wet = [1 if data > 0 else 0 for data in precipitation]\n",
    "for i in range(1, len(consecutive_wet)):\n",
    "    if consecutive_wet[i] == 1:\n",
    "        consecutive_wet[i] += consecutive_wet[i - 1]\n",
    "\n",
    "rain_t['cons_dry'] = consecutive_dry\n",
    "rain_t['cons_wet'] = consecutive_wet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x183a9eabcc0>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr_t = rain_t.corr()\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(corr_t, mask=np.zeros_like(corr_t, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_orig = rain_t['rain']  #columnas objetivo\n",
    "X_orig = rain_t.drop('rain', axis=1)  #columnas independientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = X_orig.drop('añoHid', axis=1)\n",
    "X3 = np.around(X2, decimals = 0, out = None)\n",
    "X3 = X3.astype(int)\n",
    "y3 = np.around(y_orig, decimals = 0, out = None)\n",
    "y3 = y3.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training dataset into test and train \n",
    "# (we won't be using testing sets here, because of the cross-validation; but it couldn be useful)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X3, y3, test_size=0.3, random_state=42) ##test_size conviene 0.2???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\model_selection\\_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Log Reg:  0.8500177910685153\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='auto',\n",
      "          n_jobs=None, penalty='l2', random_state=42, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=True)\n",
      "The best classifier so far is: \n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='auto',\n",
      "          n_jobs=None, penalty='l2', random_state=42, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=True)\n",
      "Log Reg tomó 47.40 segundos para 4 configuraciones de parámetros candidatos.\n",
      "El modelo con el ranking: 1\n",
      "Scores de validación Medios: 0.850 (std: 0.012)\n",
      "Parametros: {'C': 1.0, 'penalty': 'l2', 'warm_start': True}\n",
      "\n",
      "El modelo con el ranking: 1\n",
      "Scores de validación Medios: 0.850 (std: 0.012)\n",
      "Parametros: {'C': 1.0, 'penalty': 'l2', 'warm_start': False}\n",
      "\n",
      "El modelo con el ranking: 3\n",
      "Scores de validación Medios: 0.850 (std: 0.012)\n",
      "Parametros: {'C': 1.0, 'penalty': 'l1', 'warm_start': True}\n",
      "\n",
      "El modelo con el ranking: 3\n",
      "Scores de validación Medios: 0.850 (std: 0.012)\n",
      "Parametros: {'C': 1.0, 'penalty': 'l1', 'warm_start': False}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=('clf', 'best_res'))\n",
    "\n",
    "## Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(multi_class = 'auto', random_state = 42)\n",
    "lr_param = {'penalty':('l2', 'l1'), 'C':[1.0], 'warm_start':[True, False]}\n",
    "\n",
    "lr_clf = GridSearchCV(lr, lr_param, cv=5, iid=False)\n",
    "start = time()\n",
    "lr_clf.fit(X_train, y_train)\n",
    "best_lr_clf = lr_clf.best_estimator_\n",
    "print('Best Log Reg: ', lr_clf.best_score_)\n",
    "print(best_lr_clf)\n",
    "results = results.append({'clf': best_lr_clf, 'best_res': lr_clf.best_score_}, ignore_index=True)\n",
    "\n",
    "print('The best classifier so far is: ')\n",
    "print(results.loc[results['best_res'].idxmax()]['clf'])\n",
    "\n",
    "print(\"Log Reg tomó %.2f segundos para %d configuraciones de parámetros candidatos.\"\n",
    "      % (time() - start, len(lr_clf.cv_results_['params'])))\n",
    "for i in range(len(lr_clf.cv_results_['params'])+1):\n",
    "    candidatos = np.flatnonzero(lr_clf.cv_results_['rank_test_score'] == i)\n",
    "    for candidato in candidatos:\n",
    "        print(\"El modelo con el ranking: {0}\".format(i))\n",
    "        print(\"Scores de validación Medios: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  lr_clf.cv_results_['mean_test_score'][candidato],\n",
    "                  lr_clf.cv_results_['std_test_score'][candidato]))\n",
    "        print(\"Parametros: {0}\".format(lr_clf.cv_results_['params'][candidato]))\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Score para entrenamiento: 0.85\n",
      "Score para evaluación: 0.84\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "results = pd.DataFrame(columns=('clf', 'best_res'))\n",
    "\n",
    "lr_clf2 = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='auto',\n",
    "          n_jobs=None, penalty='l2', random_state=42, solver='warn',\n",
    "          tol=0.0001, verbose=0, warm_start=True)\n",
    "\n",
    "lr_clf2.fit(X_train, y_train)\n",
    "results = results.append({'clf': lr_clf2}, ignore_index=True)\n",
    "\n",
    "print('Logistic Regression:')\n",
    "print('Score para entrenamiento: %.2f' % \n",
    "      accuracy_score(y_train, lr_clf2.predict(X_train)))\n",
    "print('Score para evaluación: %.2f' %\n",
    "      accuracy_score(y_test, lr_clf2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\model_selection\\_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SGDC score:  0.844255405679211\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "       early_stopping=False, epsilon=0.1, eta0=0.1, fit_intercept=True,\n",
      "       l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "       max_iter=10000, n_iter=None, n_iter_no_change=5, n_jobs=None,\n",
      "       penalty='l1', power_t=0.5, random_state=42, shuffle=True, tol=0.001,\n",
      "       validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "The best classifier so far is: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "       early_stopping=False, epsilon=0.1, eta0=0.1, fit_intercept=True,\n",
      "       l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "       max_iter=10000, n_iter=None, n_iter_no_change=5, n_jobs=None,\n",
      "       penalty='l1', power_t=0.5, random_state=42, shuffle=True, tol=0.001,\n",
      "       validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "\n",
      "GridSearchCV tomó 1875.04 segundos para 18 configuraciones de parámetros candidatos.\n",
      "El modelo con el ranking: 1\n",
      "Scores de validación Medios: 0.844 (std: 0.014)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 2\n",
      "Scores de validación Medios: 0.843 (std: 0.014)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 3\n",
      "Scores de validación Medios: 0.842 (std: 0.011)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 4\n",
      "Scores de validación Medios: 0.842 (std: 0.011)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 10000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 5\n",
      "Scores de validación Medios: 0.841 (std: 0.010)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 6\n",
      "Scores de validación Medios: 0.841 (std: 0.012)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 7\n",
      "Scores de validación Medios: 0.841 (std: 0.011)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 8\n",
      "Scores de validación Medios: 0.841 (std: 0.011)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 10000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 9\n",
      "Scores de validación Medios: 0.841 (std: 0.011)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 10\n",
      "Scores de validación Medios: 0.841 (std: 0.011)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 11\n",
      "Scores de validación Medios: 0.841 (std: 0.011)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 12\n",
      "Scores de validación Medios: 0.841 (std: 0.012)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 13\n",
      "Scores de validación Medios: 0.841 (std: 0.011)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 14\n",
      "Scores de validación Medios: 0.841 (std: 0.012)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 15\n",
      "Scores de validación Medios: 0.840 (std: 0.013)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 16\n",
      "Scores de validación Medios: 0.840 (std: 0.011)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 17\n",
      "Scores de validación Medios: 0.781 (std: 0.123)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 18\n",
      "Scores de validación Medios: 0.769 (std: 0.159)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 10000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=('clf', 'best_res'))\n",
    "\n",
    "## Stochastic Gradient\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "#parameters_SGDC = {'loss':('hinge', 'log'), 'max_iter': [10000], 'tol': [0.001]}\n",
    "parameters_SGDC = {'loss':('hinge', 'log'), 'learning_rate':('adaptive', 'optimal', 'invscaling'),\n",
    "              'penalty':('l2', 'l1', 'elasticnet'), 'alpha':[0.0001], 'max_iter': [10000], \n",
    "              'eta0': [0.1], 'tol': [0.001]}\n",
    "SGDC = SGDClassifier(random_state=42)\n",
    "SGDC_clf = GridSearchCV(SGDC, parameters_SGDC, cv=5, iid = False, return_train_score = True)\n",
    "start = time()\n",
    "SGDC_clf.fit(X_train, y_train)\n",
    "best_SGDC_clf = SGDC_clf.best_estimator_\n",
    "\n",
    "print('Best SGDC score: ', SGDC_clf.best_score_)\n",
    "print(best_SGDC_clf)\n",
    "results = results.append({'clf': best_SGDC_clf, 'best_res': SGDC_clf.best_score_}, ignore_index=True)\n",
    "\n",
    "print('The best classifier so far is: ')\n",
    "print(results.loc[results['best_res'].idxmax()]['clf'])\n",
    "print(\"\")\n",
    "\n",
    "print(\"GridSearchCV tomó %.2f segundos para %d configuraciones de parámetros candidatos.\"\n",
    "      % (time() - start, len(SGDC_clf.cv_results_['params'])))\n",
    "for i in range(len(SGDC_clf.cv_results_['params'])+1):\n",
    "    candidatos = np.flatnonzero(SGDC_clf.cv_results_['rank_test_score'] == i)\n",
    "    for candidato in candidatos:\n",
    "        print(\"El modelo con el ranking: {0}\".format(i))\n",
    "        print(\"Scores de validación Medios: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  SGDC_clf.cv_results_['mean_test_score'][candidato],\n",
    "                  SGDC_clf.cv_results_['std_test_score'][candidato]))\n",
    "        print(\"Parametros: {0}\".format(SGDC_clf.cv_results_['params'][candidato]))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Descendent Gradient:\n",
      "Score para entrenamiento: 0.84\n",
      "Score para evaluación: 0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "results = pd.DataFrame(columns=('clf', 'best_res'))\n",
    "\n",
    "SGDC_clf2 = SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
    "       early_stopping=False, epsilon=0.1, eta0=0.1, fit_intercept=True,\n",
    "       l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
    "       max_iter=10000, n_iter=None, n_iter_no_change=5, n_jobs=None,\n",
    "       penalty='l1', power_t=0.5, random_state=42, shuffle=True, tol=0.001,\n",
    "       validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "\n",
    "SGDC_clf2.fit(X_train, y_train)\n",
    "results = results.append({'clf': SGDC_clf2}, ignore_index=True)\n",
    "\n",
    "print('Stochastic Descendent Gradient:')\n",
    "print('Score para entrenamiento: %.2f' % \n",
    "      accuracy_score(y_train, SGDC_clf2.predict(X_train)))\n",
    "print('Score para evaluación: %.2f' %\n",
    "      accuracy_score(y_test, SGDC_clf2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset: rain_junin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Aplicar tecnicas de selección y extraccion de features  \n",
    "\n",
    "2) Analizar features data / target \n",
    "\n",
    "3) dividir dataset (training, validation, test)  \n",
    "\n",
    "4) analizar y elegir el modelo mas apropiado, entrenarlo y analizar resultados\n",
    "\n",
    "5) combinar clasificadores y analizar resultados  \n",
    "\n",
    "6) evaluar predicciones de los diferentes modelos  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hr.Med(%)</th>\n",
       "      <th>Prec(mm)</th>\n",
       "      <th>Presion(mb).Est</th>\n",
       "      <th>Presion(mb).Mar</th>\n",
       "      <th>Temperatura(°C).Max</th>\n",
       "      <th>Temperatura(°C).Med</th>\n",
       "      <th>Temperatura(°C).Min</th>\n",
       "      <th>Viento(km/h).Max</th>\n",
       "      <th>Viento(km/h).Med</th>\n",
       "      <th>Vis(km)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fecha</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>71.8</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1000.1</td>\n",
       "      <td>1009.5</td>\n",
       "      <td>31.4</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>27.8</td>\n",
       "      <td>16.7</td>\n",
       "      <td>12.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02</th>\n",
       "      <td>67.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>1011.4</td>\n",
       "      <td>32.1</td>\n",
       "      <td>25.6</td>\n",
       "      <td>20.3</td>\n",
       "      <td>25.9</td>\n",
       "      <td>19.3</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03</th>\n",
       "      <td>79.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>1010.4</td>\n",
       "      <td>31.6</td>\n",
       "      <td>23.7</td>\n",
       "      <td>20.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>21.7</td>\n",
       "      <td>12.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-04</th>\n",
       "      <td>91.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>999.6</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>26.9</td>\n",
       "      <td>22.2</td>\n",
       "      <td>20.8</td>\n",
       "      <td>18.3</td>\n",
       "      <td>10.7</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-05</th>\n",
       "      <td>79.6</td>\n",
       "      <td>9.9</td>\n",
       "      <td>1002.6</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>28.4</td>\n",
       "      <td>23.6</td>\n",
       "      <td>19.8</td>\n",
       "      <td>24.1</td>\n",
       "      <td>14.4</td>\n",
       "      <td>11.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Hr.Med(%)  Prec(mm)  Presion(mb).Est  Presion(mb).Mar  \\\n",
       "Fecha                                                               \n",
       "2016-01-01       71.8       7.9           1000.1           1009.5   \n",
       "2016-01-02       67.8       0.0           1002.0           1011.4   \n",
       "2016-01-03       79.1       0.0           1001.0           1010.4   \n",
       "2016-01-04       91.5       0.0            999.6           1009.0   \n",
       "2016-01-05       79.6       9.9           1002.6           1012.0   \n",
       "\n",
       "            Temperatura(°C).Max  Temperatura(°C).Med  Temperatura(°C).Min  \\\n",
       "Fecha                                                                       \n",
       "2016-01-01                 31.4                 24.0                 18.4   \n",
       "2016-01-02                 32.1                 25.6                 20.3   \n",
       "2016-01-03                 31.6                 23.7                 20.0   \n",
       "2016-01-04                 26.9                 22.2                 20.8   \n",
       "2016-01-05                 28.4                 23.6                 19.8   \n",
       "\n",
       "            Viento(km/h).Max  Viento(km/h).Med  Vis(km)  \n",
       "Fecha                                                    \n",
       "2016-01-01              27.8              16.7     12.4  \n",
       "2016-01-02              25.9              19.3     12.8  \n",
       "2016-01-03              37.0              21.7     12.6  \n",
       "2016-01-04              18.3              10.7     10.4  \n",
       "2016-01-05              24.1              14.4     11.3  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rain_junin = pd.read_csv(\"rain_junin_usar.csv\", parse_dates = [\"Fecha\"], index_col=[0])\n",
    "rain_junin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hr.Med(%)</th>\n",
       "      <th>Prec(mm)</th>\n",
       "      <th>Presion(mb).Est</th>\n",
       "      <th>Presion(mb).Mar</th>\n",
       "      <th>Temperatura(°C).Max</th>\n",
       "      <th>Temperatura(°C).Med</th>\n",
       "      <th>Temperatura(°C).Min</th>\n",
       "      <th>Viento(km/h).Max</th>\n",
       "      <th>Viento(km/h).Med</th>\n",
       "      <th>Vis(km)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1066.000000</td>\n",
       "      <td>1050.000000</td>\n",
       "      <td>1066.000000</td>\n",
       "      <td>1066.000000</td>\n",
       "      <td>1066.000000</td>\n",
       "      <td>1066.000000</td>\n",
       "      <td>1066.000000</td>\n",
       "      <td>1065.000000</td>\n",
       "      <td>1066.000000</td>\n",
       "      <td>1066.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>69.875141</td>\n",
       "      <td>3.424095</td>\n",
       "      <td>1005.528424</td>\n",
       "      <td>1015.193621</td>\n",
       "      <td>23.837523</td>\n",
       "      <td>16.232176</td>\n",
       "      <td>10.133865</td>\n",
       "      <td>24.056901</td>\n",
       "      <td>14.788180</td>\n",
       "      <td>12.209287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.923685</td>\n",
       "      <td>11.106181</td>\n",
       "      <td>6.087682</td>\n",
       "      <td>6.271942</td>\n",
       "      <td>6.503449</td>\n",
       "      <td>5.732337</td>\n",
       "      <td>5.833879</td>\n",
       "      <td>8.268677</td>\n",
       "      <td>5.480464</td>\n",
       "      <td>2.315869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>29.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>991.100000</td>\n",
       "      <td>1000.400000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>-6.200000</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>60.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1001.400000</td>\n",
       "      <td>1010.900000</td>\n",
       "      <td>18.825000</td>\n",
       "      <td>11.600000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>18.300000</td>\n",
       "      <td>10.900000</td>\n",
       "      <td>11.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>70.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1005.100000</td>\n",
       "      <td>1014.850000</td>\n",
       "      <td>24.200000</td>\n",
       "      <td>16.450000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>24.100000</td>\n",
       "      <td>14.100000</td>\n",
       "      <td>12.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>79.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1009.600000</td>\n",
       "      <td>1019.400000</td>\n",
       "      <td>29.200000</td>\n",
       "      <td>20.800000</td>\n",
       "      <td>14.875000</td>\n",
       "      <td>29.400000</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>13.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>97.800000</td>\n",
       "      <td>151.900000</td>\n",
       "      <td>1024.800000</td>\n",
       "      <td>1034.900000</td>\n",
       "      <td>36.800000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>53.500000</td>\n",
       "      <td>38.700000</td>\n",
       "      <td>17.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Hr.Med(%)     Prec(mm)  Presion(mb).Est  Presion(mb).Mar  \\\n",
       "count  1066.000000  1050.000000      1066.000000      1066.000000   \n",
       "mean     69.875141     3.424095      1005.528424      1015.193621   \n",
       "std      12.923685    11.106181         6.087682         6.271942   \n",
       "min      29.400000     0.000000       991.100000      1000.400000   \n",
       "25%      60.900000     0.000000      1001.400000      1010.900000   \n",
       "50%      70.500000     0.000000      1005.100000      1014.850000   \n",
       "75%      79.500000     0.000000      1009.600000      1019.400000   \n",
       "max      97.800000   151.900000      1024.800000      1034.900000   \n",
       "\n",
       "       Temperatura(°C).Max  Temperatura(°C).Med  Temperatura(°C).Min  \\\n",
       "count          1066.000000          1066.000000          1066.000000   \n",
       "mean             23.837523            16.232176            10.133865   \n",
       "std               6.503449             5.732337             5.833879   \n",
       "min               8.600000             2.800000            -6.200000   \n",
       "25%              18.825000            11.600000             6.000000   \n",
       "50%              24.200000            16.450000            10.200000   \n",
       "75%              29.200000            20.800000            14.875000   \n",
       "max              36.800000            29.000000            22.500000   \n",
       "\n",
       "       Viento(km/h).Max  Viento(km/h).Med      Vis(km)  \n",
       "count       1065.000000       1066.000000  1066.000000  \n",
       "mean          24.056901         14.788180    12.209287  \n",
       "std            8.268677          5.480464     2.315869  \n",
       "min            7.600000          2.800000     2.600000  \n",
       "25%           18.300000         10.900000    11.100000  \n",
       "50%           24.100000         14.100000    12.600000  \n",
       "75%           29.400000         18.500000    13.900000  \n",
       "max           53.500000         38.700000    17.200000  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rain_junin.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hr.Med(%)</th>\n",
       "      <th>Prec(mm)</th>\n",
       "      <th>Presion(mb).Est</th>\n",
       "      <th>Presion(mb).Mar</th>\n",
       "      <th>Temperatura(°C).Max</th>\n",
       "      <th>Temperatura(°C).Med</th>\n",
       "      <th>Temperatura(°C).Min</th>\n",
       "      <th>Viento(km/h).Max</th>\n",
       "      <th>Viento(km/h).Med</th>\n",
       "      <th>Vis(km)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fecha</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>71.8</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1000.1</td>\n",
       "      <td>1009.5</td>\n",
       "      <td>31.4</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>27.8</td>\n",
       "      <td>16.7</td>\n",
       "      <td>12.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02</th>\n",
       "      <td>67.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>1011.4</td>\n",
       "      <td>32.1</td>\n",
       "      <td>25.6</td>\n",
       "      <td>20.3</td>\n",
       "      <td>25.9</td>\n",
       "      <td>19.3</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03</th>\n",
       "      <td>79.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>1010.4</td>\n",
       "      <td>31.6</td>\n",
       "      <td>23.7</td>\n",
       "      <td>20.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>21.7</td>\n",
       "      <td>12.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-04</th>\n",
       "      <td>91.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>999.6</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>26.9</td>\n",
       "      <td>22.2</td>\n",
       "      <td>20.8</td>\n",
       "      <td>18.3</td>\n",
       "      <td>10.7</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-05</th>\n",
       "      <td>79.6</td>\n",
       "      <td>9.9</td>\n",
       "      <td>1002.6</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>28.4</td>\n",
       "      <td>23.6</td>\n",
       "      <td>19.8</td>\n",
       "      <td>24.1</td>\n",
       "      <td>14.4</td>\n",
       "      <td>11.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Hr.Med(%)  Prec(mm)  Presion(mb).Est  Presion(mb).Mar  \\\n",
       "Fecha                                                               \n",
       "2016-01-01       71.8       7.9           1000.1           1009.5   \n",
       "2016-01-02       67.8       0.0           1002.0           1011.4   \n",
       "2016-01-03       79.1       0.0           1001.0           1010.4   \n",
       "2016-01-04       91.5       0.0            999.6           1009.0   \n",
       "2016-01-05       79.6       9.9           1002.6           1012.0   \n",
       "\n",
       "            Temperatura(°C).Max  Temperatura(°C).Med  Temperatura(°C).Min  \\\n",
       "Fecha                                                                       \n",
       "2016-01-01                 31.4                 24.0                 18.4   \n",
       "2016-01-02                 32.1                 25.6                 20.3   \n",
       "2016-01-03                 31.6                 23.7                 20.0   \n",
       "2016-01-04                 26.9                 22.2                 20.8   \n",
       "2016-01-05                 28.4                 23.6                 19.8   \n",
       "\n",
       "            Viento(km/h).Max  Viento(km/h).Med  Vis(km)  \n",
       "Fecha                                                    \n",
       "2016-01-01              27.8              16.7     12.4  \n",
       "2016-01-02              25.9              19.3     12.8  \n",
       "2016-01-03              37.0              21.7     12.6  \n",
       "2016-01-04              18.3              10.7     10.4  \n",
       "2016-01-05              24.1              14.4     11.3  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rain_junin = rain_junin.drop(['Prec(mm)'], axis=1)\n",
    "rain_junin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hr.Med(%)</th>\n",
       "      <th>Prec(mm)</th>\n",
       "      <th>Presion(mb).Est</th>\n",
       "      <th>Presion(mb).Mar</th>\n",
       "      <th>Temperatura(°C).Max</th>\n",
       "      <th>Temperatura(°C).Med</th>\n",
       "      <th>Temperatura(°C).Min</th>\n",
       "      <th>Viento(km/h).Max</th>\n",
       "      <th>Viento(km/h).Med</th>\n",
       "      <th>Vis(km)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1066.000000</td>\n",
       "      <td>1050.000000</td>\n",
       "      <td>1066.000000</td>\n",
       "      <td>1066.000000</td>\n",
       "      <td>1066.000000</td>\n",
       "      <td>1066.000000</td>\n",
       "      <td>1066.000000</td>\n",
       "      <td>1065.000000</td>\n",
       "      <td>1066.000000</td>\n",
       "      <td>1066.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>69.875141</td>\n",
       "      <td>3.424095</td>\n",
       "      <td>1005.528424</td>\n",
       "      <td>1015.193621</td>\n",
       "      <td>23.837523</td>\n",
       "      <td>16.232176</td>\n",
       "      <td>10.133865</td>\n",
       "      <td>24.056901</td>\n",
       "      <td>14.788180</td>\n",
       "      <td>12.209287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.923685</td>\n",
       "      <td>11.106181</td>\n",
       "      <td>6.087682</td>\n",
       "      <td>6.271942</td>\n",
       "      <td>6.503449</td>\n",
       "      <td>5.732337</td>\n",
       "      <td>5.833879</td>\n",
       "      <td>8.268677</td>\n",
       "      <td>5.480464</td>\n",
       "      <td>2.315869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>29.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>991.100000</td>\n",
       "      <td>1000.400000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>-6.200000</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>60.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1001.400000</td>\n",
       "      <td>1010.900000</td>\n",
       "      <td>18.825000</td>\n",
       "      <td>11.600000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>18.300000</td>\n",
       "      <td>10.900000</td>\n",
       "      <td>11.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>70.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1005.100000</td>\n",
       "      <td>1014.850000</td>\n",
       "      <td>24.200000</td>\n",
       "      <td>16.450000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>24.100000</td>\n",
       "      <td>14.100000</td>\n",
       "      <td>12.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>79.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1009.600000</td>\n",
       "      <td>1019.400000</td>\n",
       "      <td>29.200000</td>\n",
       "      <td>20.800000</td>\n",
       "      <td>14.875000</td>\n",
       "      <td>29.400000</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>13.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>97.800000</td>\n",
       "      <td>151.900000</td>\n",
       "      <td>1024.800000</td>\n",
       "      <td>1034.900000</td>\n",
       "      <td>36.800000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>53.500000</td>\n",
       "      <td>38.700000</td>\n",
       "      <td>17.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Hr.Med(%)     Prec(mm)  Presion(mb).Est  Presion(mb).Mar  \\\n",
       "count  1066.000000  1050.000000      1066.000000      1066.000000   \n",
       "mean     69.875141     3.424095      1005.528424      1015.193621   \n",
       "std      12.923685    11.106181         6.087682         6.271942   \n",
       "min      29.400000     0.000000       991.100000      1000.400000   \n",
       "25%      60.900000     0.000000      1001.400000      1010.900000   \n",
       "50%      70.500000     0.000000      1005.100000      1014.850000   \n",
       "75%      79.500000     0.000000      1009.600000      1019.400000   \n",
       "max      97.800000   151.900000      1024.800000      1034.900000   \n",
       "\n",
       "       Temperatura(°C).Max  Temperatura(°C).Med  Temperatura(°C).Min  \\\n",
       "count          1066.000000          1066.000000          1066.000000   \n",
       "mean             23.837523            16.232176            10.133865   \n",
       "std               6.503449             5.732337             5.833879   \n",
       "min               8.600000             2.800000            -6.200000   \n",
       "25%              18.825000            11.600000             6.000000   \n",
       "50%              24.200000            16.450000            10.200000   \n",
       "75%              29.200000            20.800000            14.875000   \n",
       "max              36.800000            29.000000            22.500000   \n",
       "\n",
       "       Viento(km/h).Max  Viento(km/h).Med      Vis(km)  \n",
       "count       1065.000000       1066.000000  1066.000000  \n",
       "mean          24.056901         14.788180    12.209287  \n",
       "std            8.268677          5.480464     2.315869  \n",
       "min            7.600000          2.800000     2.600000  \n",
       "25%           18.300000         10.900000    11.100000  \n",
       "50%           24.100000         14.100000    12.600000  \n",
       "75%           29.400000         18.500000    13.900000  \n",
       "max           53.500000         38.700000    17.200000  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rain_junin.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, previo al análisis de Features, se cuantifican el número de datos faltantes para su posterior procesamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hr.Med(%)              30\n",
       "Prec(mm)               46\n",
       "Presion(mb).Est        30\n",
       "Presion(mb).Mar        30\n",
       "Temperatura(°C).Max    30\n",
       "Temperatura(°C).Med    30\n",
       "Temperatura(°C).Min    30\n",
       "Viento(km/h).Max       31\n",
       "Viento(km/h).Med       30\n",
       "Vis(km)                30\n",
       "dtype: int64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values_count = rain_junin.isnull().sum()\n",
    "missing_values_count[missing_values_count > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los datos son tipo \"int64\" y el número de datos faltantes es del orden de 30, salvo para las precipitaciones que resultan del orden de 46. Para procesarlos, a continuación realizaremos una interpolación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rain_junin_clean = rain_junin.interpolate()\n",
    "\n",
    "missing_values_count = rain_junin_clean.isnull().sum()\n",
    "missing_values_count[missing_values_count > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No existen más datos faltantes. A continuación, realizaremos el análisis de selección de Features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una técnica de análisis de los features más sensibles es aplicar una matriz de correlaciones y plotearlo con un mapa de Heatmap para ayudar en la interpretación de los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x183a9dde518>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr = rain_junin_clean.corr()\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Hr.Med(%)', 'Prec(mm)', 'Presion(mb).Est', 'Presion(mb).Mar',\n",
       "       'Temperatura(°C).Max', 'Temperatura(°C).Med', 'Temperatura(°C).Min',\n",
       "       'Viento(km/h).Max', 'Viento(km/h).Med', 'Vis(km)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rain_junin_clean.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se seleccionarán las features más representativas a partir de \"SelectKBest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_orig = rain_junin_clean['Prec(mm)']  #columnas objetivo\n",
    "X_orig = rain_junin_clean.drop('Prec(mm)', axis=1)  #columnas independientes\n",
    "#X1 = X_orig.drop('Fecha', axis=1)\n",
    "#X1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperatura(°C).Min</th>\n",
       "      <th>Vis(km)</th>\n",
       "      <th>Temperatura(°C).Med</th>\n",
       "      <th>Temperatura(°C).Max</th>\n",
       "      <th>Presion(mb).Mar</th>\n",
       "      <th>Hr.Med(%)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fecha</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>18.4</td>\n",
       "      <td>12.4</td>\n",
       "      <td>24.0</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1009.5</td>\n",
       "      <td>71.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02</th>\n",
       "      <td>20.3</td>\n",
       "      <td>12.8</td>\n",
       "      <td>25.6</td>\n",
       "      <td>32.1</td>\n",
       "      <td>1011.4</td>\n",
       "      <td>67.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03</th>\n",
       "      <td>20.0</td>\n",
       "      <td>12.6</td>\n",
       "      <td>23.7</td>\n",
       "      <td>31.6</td>\n",
       "      <td>1010.4</td>\n",
       "      <td>79.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-04</th>\n",
       "      <td>20.8</td>\n",
       "      <td>10.4</td>\n",
       "      <td>22.2</td>\n",
       "      <td>26.9</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>91.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-05</th>\n",
       "      <td>19.8</td>\n",
       "      <td>11.3</td>\n",
       "      <td>23.6</td>\n",
       "      <td>28.4</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>79.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-06</th>\n",
       "      <td>13.6</td>\n",
       "      <td>13.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>29.1</td>\n",
       "      <td>1016.7</td>\n",
       "      <td>53.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-07</th>\n",
       "      <td>14.0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>21.4</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1015.8</td>\n",
       "      <td>60.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-08</th>\n",
       "      <td>17.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>22.5</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1011.9</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-09</th>\n",
       "      <td>17.8</td>\n",
       "      <td>13.1</td>\n",
       "      <td>21.1</td>\n",
       "      <td>30.1</td>\n",
       "      <td>1014.4</td>\n",
       "      <td>79.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-10</th>\n",
       "      <td>16.8</td>\n",
       "      <td>9.4</td>\n",
       "      <td>19.9</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1012.7</td>\n",
       "      <td>86.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11</th>\n",
       "      <td>16.5</td>\n",
       "      <td>10.2</td>\n",
       "      <td>23.5</td>\n",
       "      <td>30.3</td>\n",
       "      <td>1009.5</td>\n",
       "      <td>76.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-12</th>\n",
       "      <td>19.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>21.9</td>\n",
       "      <td>31.1</td>\n",
       "      <td>1006.4</td>\n",
       "      <td>82.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-13</th>\n",
       "      <td>17.4</td>\n",
       "      <td>13.9</td>\n",
       "      <td>22.9</td>\n",
       "      <td>29.7</td>\n",
       "      <td>1011.0</td>\n",
       "      <td>71.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-14</th>\n",
       "      <td>17.0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>24.8</td>\n",
       "      <td>31.5</td>\n",
       "      <td>1010.6</td>\n",
       "      <td>68.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-15</th>\n",
       "      <td>16.5</td>\n",
       "      <td>13.5</td>\n",
       "      <td>23.9</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1010.0</td>\n",
       "      <td>71.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-16</th>\n",
       "      <td>17.4</td>\n",
       "      <td>13.3</td>\n",
       "      <td>24.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1010.7</td>\n",
       "      <td>69.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-17</th>\n",
       "      <td>19.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>23.9</td>\n",
       "      <td>32.6</td>\n",
       "      <td>1010.1</td>\n",
       "      <td>68.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-18</th>\n",
       "      <td>18.2</td>\n",
       "      <td>11.5</td>\n",
       "      <td>23.7</td>\n",
       "      <td>29.4</td>\n",
       "      <td>1014.4</td>\n",
       "      <td>64.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-19</th>\n",
       "      <td>17.4</td>\n",
       "      <td>13.5</td>\n",
       "      <td>23.4</td>\n",
       "      <td>31.9</td>\n",
       "      <td>1013.5</td>\n",
       "      <td>69.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-20</th>\n",
       "      <td>20.0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>26.8</td>\n",
       "      <td>33.9</td>\n",
       "      <td>1011.3</td>\n",
       "      <td>68.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-21</th>\n",
       "      <td>20.7</td>\n",
       "      <td>13.9</td>\n",
       "      <td>27.4</td>\n",
       "      <td>34.6</td>\n",
       "      <td>1012.4</td>\n",
       "      <td>68.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-22</th>\n",
       "      <td>22.1</td>\n",
       "      <td>12.4</td>\n",
       "      <td>28.9</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>65.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-23</th>\n",
       "      <td>20.4</td>\n",
       "      <td>13.9</td>\n",
       "      <td>26.5</td>\n",
       "      <td>35.4</td>\n",
       "      <td>1011.5</td>\n",
       "      <td>55.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-24</th>\n",
       "      <td>20.5</td>\n",
       "      <td>12.4</td>\n",
       "      <td>25.3</td>\n",
       "      <td>32.9</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>72.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-25</th>\n",
       "      <td>16.8</td>\n",
       "      <td>11.5</td>\n",
       "      <td>22.4</td>\n",
       "      <td>34.1</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>72.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-26</th>\n",
       "      <td>18.6</td>\n",
       "      <td>13.7</td>\n",
       "      <td>22.9</td>\n",
       "      <td>29.2</td>\n",
       "      <td>1011.5</td>\n",
       "      <td>55.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-27</th>\n",
       "      <td>12.9</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.3</td>\n",
       "      <td>27.4</td>\n",
       "      <td>1014.6</td>\n",
       "      <td>50.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-28</th>\n",
       "      <td>15.5</td>\n",
       "      <td>11.5</td>\n",
       "      <td>19.8</td>\n",
       "      <td>28.3</td>\n",
       "      <td>1013.3</td>\n",
       "      <td>74.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-29</th>\n",
       "      <td>17.1</td>\n",
       "      <td>11.7</td>\n",
       "      <td>20.9</td>\n",
       "      <td>27.2</td>\n",
       "      <td>1011.9</td>\n",
       "      <td>80.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-30</th>\n",
       "      <td>18.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>23.5</td>\n",
       "      <td>28.9</td>\n",
       "      <td>1014.2</td>\n",
       "      <td>72.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-02</th>\n",
       "      <td>7.0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>15.1</td>\n",
       "      <td>21.3</td>\n",
       "      <td>1021.6</td>\n",
       "      <td>53.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-03</th>\n",
       "      <td>8.9</td>\n",
       "      <td>14.6</td>\n",
       "      <td>18.2</td>\n",
       "      <td>26.5</td>\n",
       "      <td>1013.2</td>\n",
       "      <td>49.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-04</th>\n",
       "      <td>12.8</td>\n",
       "      <td>13.9</td>\n",
       "      <td>20.0</td>\n",
       "      <td>27.5</td>\n",
       "      <td>1005.9</td>\n",
       "      <td>55.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-05</th>\n",
       "      <td>11.8</td>\n",
       "      <td>13.9</td>\n",
       "      <td>18.0</td>\n",
       "      <td>27.4</td>\n",
       "      <td>1012.6</td>\n",
       "      <td>59.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-06</th>\n",
       "      <td>13.9</td>\n",
       "      <td>13.5</td>\n",
       "      <td>17.7</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1022.3</td>\n",
       "      <td>54.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-07</th>\n",
       "      <td>11.5</td>\n",
       "      <td>13.5</td>\n",
       "      <td>17.9</td>\n",
       "      <td>25.1</td>\n",
       "      <td>1023.2</td>\n",
       "      <td>51.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-08</th>\n",
       "      <td>12.3</td>\n",
       "      <td>13.9</td>\n",
       "      <td>20.6</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>45.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-09</th>\n",
       "      <td>15.3</td>\n",
       "      <td>14.3</td>\n",
       "      <td>20.2</td>\n",
       "      <td>29.5</td>\n",
       "      <td>1009.9</td>\n",
       "      <td>57.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-10</th>\n",
       "      <td>11.4</td>\n",
       "      <td>10.6</td>\n",
       "      <td>22.3</td>\n",
       "      <td>33.6</td>\n",
       "      <td>1011.9</td>\n",
       "      <td>54.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-11</th>\n",
       "      <td>18.1</td>\n",
       "      <td>11.5</td>\n",
       "      <td>22.4</td>\n",
       "      <td>34.8</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>71.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-12</th>\n",
       "      <td>17.9</td>\n",
       "      <td>13.5</td>\n",
       "      <td>23.6</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1011.9</td>\n",
       "      <td>59.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-13</th>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18.3</td>\n",
       "      <td>29.3</td>\n",
       "      <td>1007.2</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-14</th>\n",
       "      <td>15.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>18.4</td>\n",
       "      <td>25.2</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>70.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-15</th>\n",
       "      <td>10.2</td>\n",
       "      <td>12.6</td>\n",
       "      <td>19.8</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1015.7</td>\n",
       "      <td>48.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-16</th>\n",
       "      <td>14.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>31.2</td>\n",
       "      <td>1011.1</td>\n",
       "      <td>65.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-17</th>\n",
       "      <td>18.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>22.2</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>79.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-18</th>\n",
       "      <td>11.8</td>\n",
       "      <td>14.6</td>\n",
       "      <td>18.1</td>\n",
       "      <td>23.6</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>69.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-19</th>\n",
       "      <td>15.2</td>\n",
       "      <td>11.5</td>\n",
       "      <td>20.8</td>\n",
       "      <td>26.7</td>\n",
       "      <td>1012.5</td>\n",
       "      <td>74.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-20</th>\n",
       "      <td>16.0</td>\n",
       "      <td>13.7</td>\n",
       "      <td>21.5</td>\n",
       "      <td>27.6</td>\n",
       "      <td>1006.2</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-21</th>\n",
       "      <td>16.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>21.2</td>\n",
       "      <td>27.8</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>67.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-22</th>\n",
       "      <td>16.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>21.2</td>\n",
       "      <td>27.8</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>67.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-23</th>\n",
       "      <td>16.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>21.2</td>\n",
       "      <td>27.8</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>67.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-24</th>\n",
       "      <td>16.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>21.2</td>\n",
       "      <td>27.8</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>67.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-25</th>\n",
       "      <td>16.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>21.2</td>\n",
       "      <td>27.8</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>67.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-26</th>\n",
       "      <td>16.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>21.2</td>\n",
       "      <td>27.8</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>67.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-27</th>\n",
       "      <td>16.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>21.2</td>\n",
       "      <td>27.8</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>67.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-28</th>\n",
       "      <td>16.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>21.2</td>\n",
       "      <td>27.8</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>67.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-29</th>\n",
       "      <td>16.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>21.2</td>\n",
       "      <td>27.8</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>67.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-30</th>\n",
       "      <td>16.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>21.2</td>\n",
       "      <td>27.8</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>67.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>16.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>21.2</td>\n",
       "      <td>27.8</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>67.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1096 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Temperatura(°C).Min  Vis(km)  Temperatura(°C).Med  \\\n",
       "Fecha                                                           \n",
       "2016-01-01                 18.4     12.4                 24.0   \n",
       "2016-01-02                 20.3     12.8                 25.6   \n",
       "2016-01-03                 20.0     12.6                 23.7   \n",
       "2016-01-04                 20.8     10.4                 22.2   \n",
       "2016-01-05                 19.8     11.3                 23.6   \n",
       "2016-01-06                 13.6     13.9                 21.8   \n",
       "2016-01-07                 14.0     13.9                 21.4   \n",
       "2016-01-08                 17.0     13.1                 22.5   \n",
       "2016-01-09                 17.8     13.1                 21.1   \n",
       "2016-01-10                 16.8      9.4                 19.9   \n",
       "2016-01-11                 16.5     10.2                 23.5   \n",
       "2016-01-12                 19.0     10.4                 21.9   \n",
       "2016-01-13                 17.4     13.9                 22.9   \n",
       "2016-01-14                 17.0     13.9                 24.8   \n",
       "2016-01-15                 16.5     13.5                 23.9   \n",
       "2016-01-16                 17.4     13.3                 24.1   \n",
       "2016-01-17                 19.0     15.2                 23.9   \n",
       "2016-01-18                 18.2     11.5                 23.7   \n",
       "2016-01-19                 17.4     13.5                 23.4   \n",
       "2016-01-20                 20.0     13.9                 26.8   \n",
       "2016-01-21                 20.7     13.9                 27.4   \n",
       "2016-01-22                 22.1     12.4                 28.9   \n",
       "2016-01-23                 20.4     13.9                 26.5   \n",
       "2016-01-24                 20.5     12.4                 25.3   \n",
       "2016-01-25                 16.8     11.5                 22.4   \n",
       "2016-01-26                 18.6     13.7                 22.9   \n",
       "2016-01-27                 12.9     13.0                 20.3   \n",
       "2016-01-28                 15.5     11.5                 19.8   \n",
       "2016-01-29                 17.1     11.7                 20.9   \n",
       "2016-01-30                 18.2     10.0                 23.5   \n",
       "...                         ...      ...                  ...   \n",
       "2018-12-02                  7.0     13.9                 15.1   \n",
       "2018-12-03                  8.9     14.6                 18.2   \n",
       "2018-12-04                 12.8     13.9                 20.0   \n",
       "2018-12-05                 11.8     13.9                 18.0   \n",
       "2018-12-06                 13.9     13.5                 17.7   \n",
       "2018-12-07                 11.5     13.5                 17.9   \n",
       "2018-12-08                 12.3     13.9                 20.6   \n",
       "2018-12-09                 15.3     14.3                 20.2   \n",
       "2018-12-10                 11.4     10.6                 22.3   \n",
       "2018-12-11                 18.1     11.5                 22.4   \n",
       "2018-12-12                 17.9     13.5                 23.6   \n",
       "2018-12-13                 15.0     10.0                 18.3   \n",
       "2018-12-14                 15.0     11.5                 18.4   \n",
       "2018-12-15                 10.2     12.6                 19.8   \n",
       "2018-12-16                 14.0     13.1                 23.0   \n",
       "2018-12-17                 18.0     13.1                 22.2   \n",
       "2018-12-18                 11.8     14.6                 18.1   \n",
       "2018-12-19                 15.2     11.5                 20.8   \n",
       "2018-12-20                 16.0     13.7                 21.5   \n",
       "2018-12-21                 16.0     12.8                 21.2   \n",
       "2018-12-22                 16.0     12.8                 21.2   \n",
       "2018-12-23                 16.0     12.8                 21.2   \n",
       "2018-12-24                 16.0     12.8                 21.2   \n",
       "2018-12-25                 16.0     12.8                 21.2   \n",
       "2018-12-26                 16.0     12.8                 21.2   \n",
       "2018-12-27                 16.0     12.8                 21.2   \n",
       "2018-12-28                 16.0     12.8                 21.2   \n",
       "2018-12-29                 16.0     12.8                 21.2   \n",
       "2018-12-30                 16.0     12.8                 21.2   \n",
       "2018-12-31                 16.0     12.8                 21.2   \n",
       "\n",
       "            Temperatura(°C).Max  Presion(mb).Mar  Hr.Med(%)  \n",
       "Fecha                                                        \n",
       "2016-01-01                 31.4           1009.5       71.8  \n",
       "2016-01-02                 32.1           1011.4       67.8  \n",
       "2016-01-03                 31.6           1010.4       79.1  \n",
       "2016-01-04                 26.9           1009.0       91.5  \n",
       "2016-01-05                 28.4           1012.0       79.6  \n",
       "2016-01-06                 29.1           1016.7       53.2  \n",
       "2016-01-07                 28.0           1015.8       60.1  \n",
       "2016-01-08                 29.0           1011.9       66.0  \n",
       "2016-01-09                 30.1           1014.4       79.6  \n",
       "2016-01-10                 25.0           1012.7       86.7  \n",
       "2016-01-11                 30.3           1009.5       76.4  \n",
       "2016-01-12                 31.1           1006.4       82.8  \n",
       "2016-01-13                 29.7           1011.0       71.2  \n",
       "2016-01-14                 31.5           1010.6       68.3  \n",
       "2016-01-15                 32.0           1010.0       71.3  \n",
       "2016-01-16                 32.0           1010.7       69.2  \n",
       "2016-01-17                 32.6           1010.1       68.9  \n",
       "2016-01-18                 29.4           1014.4       64.2  \n",
       "2016-01-19                 31.9           1013.5       69.3  \n",
       "2016-01-20                 33.9           1011.3       68.7  \n",
       "2016-01-21                 34.6           1012.4       68.6  \n",
       "2016-01-22                 35.0           1008.4       65.5  \n",
       "2016-01-23                 35.4           1011.5       55.7  \n",
       "2016-01-24                 32.9           1003.0       72.1  \n",
       "2016-01-25                 34.1           1006.0       72.1  \n",
       "2016-01-26                 29.2           1011.5       55.8  \n",
       "2016-01-27                 27.4           1014.6       50.5  \n",
       "2016-01-28                 28.3           1013.3       74.5  \n",
       "2016-01-29                 27.2           1011.9       80.1  \n",
       "2016-01-30                 28.9           1014.2       72.5  \n",
       "...                         ...              ...        ...  \n",
       "2018-12-02                 21.3           1021.6       53.6  \n",
       "2018-12-03                 26.5           1013.2       49.5  \n",
       "2018-12-04                 27.5           1005.9       55.5  \n",
       "2018-12-05                 27.4           1012.6       59.1  \n",
       "2018-12-06                 25.0           1022.3       54.1  \n",
       "2018-12-07                 25.1           1023.2       51.4  \n",
       "2018-12-08                 29.0           1016.0       45.7  \n",
       "2018-12-09                 29.5           1009.9       57.7  \n",
       "2018-12-10                 33.6           1011.9       54.3  \n",
       "2018-12-11                 34.8           1007.9       71.1  \n",
       "2018-12-12                 28.0           1011.9       59.1  \n",
       "2018-12-13                 29.3           1007.2       85.0  \n",
       "2018-12-14                 25.2           1006.0       70.8  \n",
       "2018-12-15                 27.0           1015.7       48.7  \n",
       "2018-12-16                 31.2           1011.1       65.8  \n",
       "2018-12-17                 32.0           1009.0       79.2  \n",
       "2018-12-18                 23.6           1018.0       69.5  \n",
       "2018-12-19                 26.7           1012.5       74.4  \n",
       "2018-12-20                 27.6           1006.2       75.0  \n",
       "2018-12-21                 27.8           1007.8       67.4  \n",
       "2018-12-22                 27.8           1007.8       67.4  \n",
       "2018-12-23                 27.8           1007.8       67.4  \n",
       "2018-12-24                 27.8           1007.8       67.4  \n",
       "2018-12-25                 27.8           1007.8       67.4  \n",
       "2018-12-26                 27.8           1007.8       67.4  \n",
       "2018-12-27                 27.8           1007.8       67.4  \n",
       "2018-12-28                 27.8           1007.8       67.4  \n",
       "2018-12-29                 27.8           1007.8       67.4  \n",
       "2018-12-30                 27.8           1007.8       67.4  \n",
       "2018-12-31                 27.8           1007.8       67.4  \n",
       "\n",
       "[1096 rows x 6 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DecisionTreeRegressor()\n",
    "model.fit(X_orig,y_orig)\n",
    "\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X_orig.columns)\n",
    "feat_importances.nlargest(9).plot(kind='barh')\n",
    "f1 = feat_importances.nlargest(6)\n",
    "plt.show()\n",
    "X2 = X_orig[f1.index]\n",
    "X2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se normalizan los valores anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01816271, 0.01224009, 0.02369049, 0.03099506, 0.99648142,\n",
       "        0.07087406],\n",
       "       [0.02000424, 0.01261351, 0.02522702, 0.03163232, 0.99666436,\n",
       "        0.06681219],\n",
       "       [0.01971344, 0.01241947, 0.02336042, 0.03114723, 0.99592293,\n",
       "        0.07796665],\n",
       "       ...,\n",
       "       [0.01582805, 0.01266244, 0.02097216, 0.02750123, 0.99696908,\n",
       "        0.06667565],\n",
       "       [0.01582805, 0.01266244, 0.02097216, 0.02750123, 0.99696908,\n",
       "        0.06667565],\n",
       "       [0.01582805, 0.01266244, 0.02097216, 0.02750123, 0.99696908,\n",
       "        0.06667565]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = Normalizer().fit(X2)\n",
    "#scaler = StandardScaler().fit(X2)\n",
    "std_X2 = scaler.transform(X2)\n",
    "std_X2\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc = StandardScaler()\n",
    "# sc_X_train = sc.fit_transform(X_train)\n",
    "# sc_X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como arrojaban errores de que no podía ajustarse el algoritmo de regresión lineal o logística a los datos anteriores por ser del tipo flotante, se los redondeó de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = np.around(X2, decimals = 0, out = None)\n",
    "X3 = X3.astype(int)\n",
    "y3 = np.around(y_orig, decimals = 0, out = None)\n",
    "y3 = y3.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los set de train y test se repartieron en 70% y 30% respectivamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training dataset into test and train \n",
    "# (we won't be using testing sets here, because of the cross-validation; but it couldn be useful)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X3, y3, test_size=0.3, random_state=42) ##test_size conviene 0.2???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regresión Logística dió muy buenos resultados. Cabe resaltar que el accuracy es una métrica de Clasificación y en este caso estamos lidiando con Regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\model_selection\\_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Log Reg:  0.7905001047987451\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='auto',\n",
      "          n_jobs=None, penalty='l1', random_state=42, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=True)\n",
      "The best classifier so far is: \n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='auto',\n",
      "          n_jobs=None, penalty='l1', random_state=42, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=True)\n",
      "Log Reg tomó 4.23 segundos para 4 configuraciones de parámetros candidatos.\n",
      "El modelo con el ranking: 1\n",
      "Scores de validación Medios: 0.791 (std: 0.072)\n",
      "Parametros: {'C': 1.0, 'penalty': 'l1', 'warm_start': True}\n",
      "\n",
      "El modelo con el ranking: 1\n",
      "Scores de validación Medios: 0.791 (std: 0.072)\n",
      "Parametros: {'C': 1.0, 'penalty': 'l1', 'warm_start': False}\n",
      "\n",
      "El modelo con el ranking: 3\n",
      "Scores de validación Medios: 0.786 (std: 0.070)\n",
      "Parametros: {'C': 1.0, 'penalty': 'l2', 'warm_start': True}\n",
      "\n",
      "El modelo con el ranking: 3\n",
      "Scores de validación Medios: 0.786 (std: 0.070)\n",
      "Parametros: {'C': 1.0, 'penalty': 'l2', 'warm_start': False}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=('clf', 'best_res'))\n",
    "\n",
    "## Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(multi_class = 'auto', random_state = 42)\n",
    "lr_param = {'penalty':('l2', 'l1'), 'C':[1.0], 'warm_start':[True, False]}\n",
    "\n",
    "lr_clf = GridSearchCV(lr, lr_param, cv=5, iid=False)\n",
    "start = time()\n",
    "lr_clf.fit(X_train, y_train)\n",
    "best_lr_clf = lr_clf.best_estimator_\n",
    "print('Best Log Reg: ', lr_clf.best_score_)\n",
    "print(best_lr_clf)\n",
    "results = results.append({'clf': best_lr_clf, 'best_res': lr_clf.best_score_}, ignore_index=True)\n",
    "\n",
    "print('The best classifier so far is: ')\n",
    "print(results.loc[results['best_res'].idxmax()]['clf'])\n",
    "\n",
    "print(\"Log Reg tomó %.2f segundos para %d configuraciones de parámetros candidatos.\"\n",
    "      % (time() - start, len(lr_clf.cv_results_['params'])))\n",
    "for i in range(len(lr_clf.cv_results_['params'])+1):\n",
    "    candidatos = np.flatnonzero(lr_clf.cv_results_['rank_test_score'] == i)\n",
    "    for candidato in candidatos:\n",
    "        print(\"El modelo con el ranking: {0}\".format(i))\n",
    "        print(\"Scores de validación Medios: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  lr_clf.cv_results_['mean_test_score'][candidato],\n",
    "                  lr_clf.cv_results_['std_test_score'][candidato]))\n",
    "        print(\"Parametros: {0}\".format(lr_clf.cv_results_['params'][candidato]))\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Score para entrenamiento: 0.79\n",
      "Score para evaluación: 0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "results = pd.DataFrame(columns=('clf', 'best_res'))\n",
    "\n",
    "lr_clf2 = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='auto',\n",
    "          n_jobs=None, penalty='l2', random_state=42, solver='warn',\n",
    "          tol=0.0001, verbose=0, warm_start=True)\n",
    "\n",
    "lr_clf2.fit(X_train, y_train)\n",
    "results = results.append({'clf': lr_clf2}, ignore_index=True)\n",
    "\n",
    "print('Logistic Regression:')\n",
    "print('Score para entrenamiento: %.2f' % \n",
    "      accuracy_score(y_train, lr_clf2.predict(X_train)))\n",
    "print('Score para evaluación: %.2f' %\n",
    "      accuracy_score(y_test, lr_clf2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\model_selection\\_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SGDC score:  0.7339747803413788\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "       early_stopping=False, epsilon=0.1, eta0=0.1, fit_intercept=True,\n",
      "       l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "       max_iter=10000, n_iter=None, n_iter_no_change=5, n_jobs=None,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=42, shuffle=True,\n",
      "       tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "The best classifier so far is: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "       early_stopping=False, epsilon=0.1, eta0=0.1, fit_intercept=True,\n",
      "       l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "       max_iter=10000, n_iter=None, n_iter_no_change=5, n_jobs=None,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=42, shuffle=True,\n",
      "       tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "\n",
      "GridSearchCV tomó 26.98 segundos para 18 configuraciones de parámetros candidatos.\n",
      "El modelo con el ranking: 1\n",
      "Scores de validación Medios: 0.734 (std: 0.050)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 2\n",
      "Scores de validación Medios: 0.713 (std: 0.088)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 3\n",
      "Scores de validación Medios: 0.695 (std: 0.109)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 10000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 4\n",
      "Scores de validación Medios: 0.687 (std: 0.045)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 10000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 5\n",
      "Scores de validación Medios: 0.685 (std: 0.047)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 6\n",
      "Scores de validación Medios: 0.685 (std: 0.044)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 7\n",
      "Scores de validación Medios: 0.683 (std: 0.042)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 7\n",
      "Scores de validación Medios: 0.683 (std: 0.042)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 9\n",
      "Scores de validación Medios: 0.682 (std: 0.047)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 10\n",
      "Scores de validación Medios: 0.681 (std: 0.171)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 10000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 11\n",
      "Scores de validación Medios: 0.681 (std: 0.135)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 12\n",
      "Scores de validación Medios: 0.660 (std: 0.202)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 13\n",
      "Scores de validación Medios: 0.638 (std: 0.117)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 14\n",
      "Scores de validación Medios: 0.596 (std: 0.304)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 15\n",
      "Scores de validación Medios: 0.582 (std: 0.281)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 16\n",
      "Scores de validación Medios: 0.565 (std: 0.238)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 17\n",
      "Scores de validación Medios: 0.556 (std: 0.266)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\n",
      "El modelo con el ranking: 18\n",
      "Scores de validación Medios: 0.538 (std: 0.232)\n",
      "Parametros: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=('clf', 'best_res'))\n",
    "\n",
    "## Stochastic Gradient\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "#parameters_SGDC = {'loss':('hinge', 'log'), 'max_iter': [10000], 'tol': [0.001]}\n",
    "parameters_SGDC = {'loss':('hinge', 'log'), 'learning_rate':('adaptive', 'optimal', 'invscaling'),\n",
    "              'penalty':('l2', 'l1', 'elasticnet'), 'alpha':[0.0001], 'max_iter': [10000], \n",
    "              'eta0': [0.1], 'tol': [0.001]}\n",
    "SGDC = SGDClassifier(random_state=42)\n",
    "SGDC_clf = GridSearchCV(SGDC, parameters_SGDC, cv=5, iid = False, return_train_score = True)\n",
    "start = time()\n",
    "SGDC_clf.fit(X_train, y_train)\n",
    "best_SGDC_clf = SGDC_clf.best_estimator_\n",
    "\n",
    "print('Best SGDC score: ', SGDC_clf.best_score_)\n",
    "print(best_SGDC_clf)\n",
    "results = results.append({'clf': best_SGDC_clf, 'best_res': SGDC_clf.best_score_}, ignore_index=True)\n",
    "\n",
    "print('The best classifier so far is: ')\n",
    "print(results.loc[results['best_res'].idxmax()]['clf'])\n",
    "print(\"\")\n",
    "\n",
    "print(\"GridSearchCV tomó %.2f segundos para %d configuraciones de parámetros candidatos.\"\n",
    "      % (time() - start, len(SGDC_clf.cv_results_['params'])))\n",
    "for i in range(len(SGDC_clf.cv_results_['params'])+1):\n",
    "    candidatos = np.flatnonzero(SGDC_clf.cv_results_['rank_test_score'] == i)\n",
    "    for candidato in candidatos:\n",
    "        print(\"El modelo con el ranking: {0}\".format(i))\n",
    "        print(\"Scores de validación Medios: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  SGDC_clf.cv_results_['mean_test_score'][candidato],\n",
    "                  SGDC_clf.cv_results_['std_test_score'][candidato]))\n",
    "        print(\"Parametros: {0}\".format(SGDC_clf.cv_results_['params'][candidato]))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Descendent Gradient:\n",
      "Score para entrenamiento: 0.68\n",
      "Score para evaluación: 0.66\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "results = pd.DataFrame(columns=('clf', 'best_res'))\n",
    "\n",
    "SGDC_clf2 = SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
    "       early_stopping=False, epsilon=0.1, eta0=0.1, fit_intercept=True,\n",
    "       l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
    "       max_iter=10000, n_iter=None, n_iter_no_change=5, n_jobs=None,\n",
    "       penalty='l2', power_t=0.5, random_state=42, shuffle=True, tol=0.001,\n",
    "       validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "\n",
    "SGDC_clf2.fit(X_train, y_train)\n",
    "results = results.append({'clf': SGDC_clf2}, ignore_index=True)\n",
    "\n",
    "print('Stochastic Descendent Gradient:')\n",
    "print('Score para entrenamiento: %.2f' % \n",
    "      accuracy_score(y_train, SGDC_clf2.predict(X_train)))\n",
    "print('Score para evaluación: %.2f' %\n",
    "      accuracy_score(y_test, SGDC_clf2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Log Reg:  0.10817753967797392\n",
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
      "         normalize=False)\n",
      "The best classifier so far is: \n",
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
      "         normalize=False)\n",
      "Log Reg tomó 0.12 segundos para 1 configuraciones de parámetros candidatos.\n",
      "El modelo con el ranking: 1\n",
      "Scores de validación Medios: 0.108 (std: 0.047)\n",
      "Parametros: {}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=('clf', 'best_res'))\n",
    "\n",
    "## Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr_param = {}\n",
    "\n",
    "lr_clf = GridSearchCV(lr, lr_param, cv=5, iid=False)\n",
    "start = time()\n",
    "lr_clf.fit(X_train, y_train)\n",
    "best_lr_clf = lr_clf.best_estimator_\n",
    "print('Best Log Reg: ', lr_clf.best_score_)\n",
    "print(best_lr_clf)\n",
    "results = results.append({'clf': best_lr_clf, 'best_res': lr_clf.best_score_}, ignore_index=True)\n",
    "\n",
    "print('The best classifier so far is: ')\n",
    "print(results.loc[results['best_res'].idxmax()]['clf'])\n",
    "\n",
    "print(\"Log Reg tomó %.2f segundos para %d configuraciones de parámetros candidatos.\"\n",
    "      % (time() - start, len(lr_clf.cv_results_['params'])))\n",
    "for i in range(len(lr_clf.cv_results_['params'])+1):\n",
    "    candidatos = np.flatnonzero(lr_clf.cv_results_['rank_test_score'] == i)\n",
    "    for candidato in candidatos:\n",
    "        print(\"El modelo con el ranking: {0}\".format(i))\n",
    "        print(\"Scores de validación Medios: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  lr_clf.cv_results_['mean_test_score'][candidato],\n",
    "                  lr_clf.cv_results_['std_test_score'][candidato]))\n",
    "        print(\"Parametros: {0}\".format(lr_clf.cv_results_['params'][candidato]))\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Decision Tree:  0.013349638640077311\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=16,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=None,\n",
      "           oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "The best classifier so far is: \n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=16,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=None,\n",
      "           oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "El modelo con el ranking: 1\n",
      "Scores de validación Medios: 0.013 (std: 0.208)\n",
      "Parametros: {'bootstrap': True, 'max_depth': 2, 'min_samples_split': 16, 'warm_start': False}\n",
      "\n",
      "El modelo con el ranking: 2\n",
      "Scores de validación Medios: -0.015 (std: 0.209)\n",
      "Parametros: {'bootstrap': False, 'max_depth': 1, 'min_samples_split': 22, 'warm_start': True}\n",
      "\n",
      "El modelo con el ranking: 3\n",
      "Scores de validación Medios: -0.021 (std: 0.125)\n",
      "Parametros: {'bootstrap': True, 'max_depth': 1, 'min_samples_split': 6, 'warm_start': False}\n",
      "\n",
      "El modelo con el ranking: 3\n",
      "Scores de validación Medios: -0.021 (std: 0.125)\n",
      "Parametros: {'bootstrap': True, 'max_depth': 1, 'min_samples_split': 7, 'warm_start': True}\n",
      "\n",
      "El modelo con el ranking: 5\n",
      "Scores de validación Medios: -0.037 (std: 0.306)\n",
      "Parametros: {'bootstrap': False, 'max_depth': 2, 'min_samples_split': 21, 'warm_start': True}\n",
      "\n",
      "El modelo con el ranking: 5\n",
      "Scores de validación Medios: -0.037 (std: 0.306)\n",
      "Parametros: {'bootstrap': False, 'max_depth': 2, 'min_samples_split': 24, 'warm_start': True}\n",
      "\n",
      "El modelo con el ranking: 7\n",
      "Scores de validación Medios: -0.065 (std: 0.440)\n",
      "Parametros: {'bootstrap': True, 'max_depth': 4, 'min_samples_split': 17, 'warm_start': False}\n",
      "\n",
      "El modelo con el ranking: 8\n",
      "Scores de validación Medios: -0.066 (std: 0.439)\n",
      "Parametros: {'bootstrap': True, 'max_depth': 4, 'min_samples_split': 19, 'warm_start': True}\n",
      "\n",
      "El modelo con el ranking: 9\n",
      "Scores de validación Medios: -0.107 (std: 0.548)\n",
      "Parametros: {'bootstrap': True, 'max_depth': 23, 'min_samples_split': 24, 'warm_start': False}\n",
      "\n",
      "El modelo con el ranking: 10\n",
      "Scores de validación Medios: -0.114 (std: 0.544)\n",
      "Parametros: {'bootstrap': True, 'max_depth': 11, 'min_samples_split': 19, 'warm_start': False}\n",
      "\n",
      "El modelo con el ranking: 11\n",
      "Scores de validación Medios: -0.120 (std: 0.562)\n",
      "Parametros: {'bootstrap': True, 'max_depth': 23, 'min_samples_split': 20, 'warm_start': False}\n",
      "\n",
      "El modelo con el ranking: 12\n",
      "Scores de validación Medios: -0.129 (std: 0.567)\n",
      "Parametros: {'bootstrap': True, 'max_depth': 6, 'min_samples_split': 13, 'warm_start': True}\n",
      "\n",
      "El modelo con el ranking: 13\n",
      "Scores de validación Medios: -0.136 (std: 0.604)\n",
      "Parametros: {'bootstrap': True, 'max_depth': 5, 'min_samples_split': 8, 'warm_start': True}\n",
      "\n",
      "El modelo con el ranking: 14\n",
      "Scores de validación Medios: -0.173 (std: 0.631)\n",
      "Parametros: {'bootstrap': True, 'max_depth': 16, 'min_samples_split': 11, 'warm_start': True}\n",
      "\n",
      "El modelo con el ranking: 15\n",
      "Scores de validación Medios: -0.215 (std: 0.740)\n",
      "Parametros: {'bootstrap': True, 'max_depth': 15, 'min_samples_split': 9, 'warm_start': True}\n",
      "\n",
      "El modelo con el ranking: 16\n",
      "Scores de validación Medios: -0.215 (std: 0.740)\n",
      "Parametros: {'bootstrap': True, 'max_depth': 18, 'min_samples_split': 9, 'warm_start': False}\n",
      "\n",
      "El modelo con el ranking: 17\n",
      "Scores de validación Medios: -0.217 (std: 0.775)\n",
      "Parametros: {'bootstrap': False, 'max_depth': 9, 'min_samples_split': 24, 'warm_start': True}\n",
      "\n",
      "El modelo con el ranking: 18\n",
      "Scores de validación Medios: -0.224 (std: 0.782)\n",
      "Parametros: {'bootstrap': True, 'max_depth': 16, 'min_samples_split': 7, 'warm_start': True}\n",
      "\n",
      "El modelo con el ranking: 19\n",
      "Scores de validación Medios: -0.224 (std: 0.782)\n",
      "Parametros: {'bootstrap': True, 'max_depth': 15, 'min_samples_split': 7, 'warm_start': True}\n",
      "\n",
      "El modelo con el ranking: 20\n",
      "Scores de validación Medios: -0.240 (std: 0.759)\n",
      "Parametros: {'bootstrap': False, 'max_depth': 24, 'min_samples_split': 23, 'warm_start': False}\n",
      "\n",
      "El modelo con el ranking: 21\n",
      "Scores de validación Medios: -0.302 (std: 0.914)\n",
      "Parametros: {'bootstrap': False, 'max_depth': 10, 'min_samples_split': 22, 'warm_start': True}\n",
      "\n",
      "El modelo con el ranking: 22\n",
      "Scores de validación Medios: -0.303 (std: 0.914)\n",
      "Parametros: {'bootstrap': False, 'max_depth': 15, 'min_samples_split': 22, 'warm_start': True}\n",
      "\n",
      "El modelo con el ranking: 23\n",
      "Scores de validación Medios: -0.307 (std: 0.912)\n",
      "Parametros: {'bootstrap': False, 'max_depth': 6, 'min_samples_split': 19, 'warm_start': False}\n",
      "\n",
      "El modelo con el ranking: 24\n",
      "Scores de validación Medios: -0.335 (std: 0.923)\n",
      "Parametros: {'bootstrap': False, 'max_depth': 23, 'min_samples_split': 18, 'warm_start': True}\n",
      "\n",
      "El modelo con el ranking: 25\n",
      "Scores de validación Medios: -0.354 (std: 0.928)\n",
      "Parametros: {'bootstrap': False, 'max_depth': 21, 'min_samples_split': 17, 'warm_start': True}\n",
      "\n",
      "El modelo con el ranking: 26\n",
      "Scores de validación Medios: -0.367 (std: 0.954)\n",
      "Parametros: {'bootstrap': False, 'max_depth': 24, 'min_samples_split': 14, 'warm_start': False}\n",
      "\n",
      "El modelo con el ranking: 27\n",
      "Scores de validación Medios: -0.894 (std: 2.425)\n",
      "Parametros: {'bootstrap': False, 'max_depth': 6, 'min_samples_split': 9, 'warm_start': False}\n",
      "\n",
      "El modelo con el ranking: 28\n",
      "Scores de validación Medios: -0.994 (std: 2.268)\n",
      "Parametros: {'bootstrap': False, 'max_depth': 12, 'min_samples_split': 11, 'warm_start': True}\n",
      "\n",
      "El modelo con el ranking: 29\n",
      "Scores de validación Medios: -1.354 (std: 2.487)\n",
      "Parametros: {'bootstrap': False, 'max_depth': 17, 'min_samples_split': 6, 'warm_start': False}\n",
      "\n",
      "El modelo con el ranking: 30\n",
      "Scores de validación Medios: -1.749 (std: 3.459)\n",
      "Parametros: {'bootstrap': False, 'max_depth': 18, 'min_samples_split': 2, 'warm_start': False}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=('clf', 'best_res'))\n",
    "\n",
    "## Random Forest - Regression\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR #Javier: Leer y probar otros parámetros.\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "tree_param_dist = {'max_depth': sp_randint(1, 25),\n",
    "#                    'max_features': sp_randint(4, 10),\n",
    "                   'min_samples_split': sp_randint(2, 25),\n",
    "                   'bootstrap': [True, False],\n",
    "                   'warm_start': [True, False],}\n",
    "#                    'criterion': ['gini', 'entropy']}\n",
    "\n",
    "# tree_param_dist = {}\n",
    "tree = RFR(random_state=42, n_estimators=30, min_samples_leaf=1)\n",
    "\n",
    "tree_clf = RandomizedSearchCV(tree, param_distributions=tree_param_dist, n_iter=30, cv=10, iid=False) \n",
    "\n",
    "# start = time()\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "best_tree_clf = tree_clf.best_estimator_\n",
    "print('Best Decision Tree: ', tree_clf.best_score_)\n",
    "print(best_tree_clf)\n",
    "results = results.append({'clf': best_tree_clf, 'best_res': tree_clf.best_score_}, ignore_index=True)\n",
    "\n",
    "print('The best classifier so far is: ')\n",
    "print(results.loc[results['best_res'].idxmax()]['clf'])\n",
    "\n",
    "# print(\"Regression Tree tomó %.2f segundos para %d configuraciones de parámetros candidatos.\"\n",
    "#       % (time() - start, len(tree_clf.cv_results_['params'])))\n",
    "for i in range(len(tree_clf.cv_results_['params'])+1):\n",
    "    candidatos = np.flatnonzero(tree_clf.cv_results_['rank_test_score'] == i)\n",
    "    for candidato in candidatos:\n",
    "        print(\"El modelo con el ranking: {0}\".format(i))\n",
    "        print(\"Scores de validación Medios: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  tree_clf.cv_results_['mean_test_score'][candidato],\n",
    "                  tree_clf.cv_results_['std_test_score'][candidato]))\n",
    "        print(\"Parametros: {0}\".format(tree_clf.cv_results_['params'][candidato]))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\model_selection\\_search.py:271: UserWarning: The total space of parameters 1 is smaller than n_iter=30. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Linear SVR:  -0.28394465419300835\n",
      "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
      "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
      "     random_state=42, tol=0.0001, verbose=0)\n",
      "The best classifier so far is: \n",
      "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
      "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
      "     random_state=42, tol=0.0001, verbose=0)\n",
      "Linear SVR tomó 0.27 segundos para 1 configuraciones de parámetros candidatos.\n",
      "El modelo con el ranking: 1\n",
      "Scores de validación Medios: -0.284 (std: 0.430)\n",
      "Parametros: {}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\javi\\.conda\\envs\\diplodatos\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=('clf', 'best_res'))\n",
    "\n",
    "## Linear SVC\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "lsvr = LinearSVR(random_state=42)\n",
    "lsvr_param = {}\n",
    "\n",
    "lsvr_clf = RandomizedSearchCV(lsvr, lsvr_param, n_iter=30, cv=5, iid=False)\n",
    "start = time()\n",
    "lsvr_clf.fit(X_train, y_train)\n",
    "best_lsvr_clf = lsvr_clf.best_estimator_\n",
    "print('Best Linear SVR: ', lsvr_clf.best_score_)\n",
    "print(best_lsvr_clf)\n",
    "results = results.append({'clf': best_lsvr_clf, 'best_res': lsvr_clf.best_score_}, ignore_index=True)\n",
    "\n",
    "print('The best classifier so far is: ')\n",
    "print(results.loc[results['best_res'].idxmax()]['clf'])\n",
    "\n",
    "print(\"Linear SVR tomó %.2f segundos para %d configuraciones de parámetros candidatos.\"\n",
    "      % (time() - start, len(lsvr_clf.cv_results_['params'])))\n",
    "for i in range(len(lsvr_clf.cv_results_['params'])+1):\n",
    "    candidatos = np.flatnonzero(lsvr_clf.cv_results_['rank_test_score'] == i)\n",
    "    for candidato in candidatos:\n",
    "        print(\"El modelo con el ranking: {0}\".format(i))\n",
    "        print(\"Scores de validación Medios: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  lsvr_clf.cv_results_['mean_test_score'][candidato],\n",
    "                  lsvr_clf.cv_results_['std_test_score'][candidato]))\n",
    "        print(\"Parametros: {0}\".format(lsvr_clf.cv_results_['params'][candidato]))\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combinacion de datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Combinar ambos datasets a partir de features comunes a ambos\n",
    "\n",
    "2) Validar la correcctitud de los datos fusionados\n",
    "\n",
    "3) Aplicar tecnicas de selección y extraccion de features  \n",
    "\n",
    "4) Analizar features data / target \n",
    "\n",
    "5) dividir dataset (training, validation, test)  \n",
    "\n",
    "6) analizar y elegir el modelo mas apropiado, entrenarlo y analizar resultados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
